{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbriggs/opt/anaconda3/envs/graphai/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/jamesbriggs/opt/anaconda3/envs/graphai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from semantic_router.llms import OpenAILLM\n",
    "\n",
    "llm = OpenAILLM(name=\"gpt-4o-2024-08-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Search(BaseModel):\n",
    "    query: str = Field(description=\"Search query for internet information\")\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    query: str = Field(description=\"Self-directed query to search information from your long term memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=False\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=False\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:39 INFO semantic_router.utils.logger JB TEMP !!!: stream=False\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import openai\n",
    "from graphai import router, node\n",
    "from semantic_router.schema import Message\n",
    "\n",
    "\n",
    "@node(start=True)\n",
    "def node_start(input: dict):\n",
    "    \"\"\"Descriptive string for the node.\"\"\"\n",
    "    print(\"node_a\")\n",
    "    return {\"input\": input}\n",
    "\n",
    "\n",
    "@router\n",
    "def node_router(input: dict):\n",
    "    print(\"node_router\")\n",
    "    query = input[\"query\"]\n",
    "    messages = [\n",
    "        Message(\n",
    "            role=\"system\",\n",
    "            content=\"\"\"You are a helpful assistant. Select the best route to answer the user query. ONLY choose one function.\"\"\",\n",
    "        ),\n",
    "        Message(role=\"user\", content=query),\n",
    "    ]\n",
    "    response = llm(\n",
    "        messages=messages,\n",
    "        function_schemas=[\n",
    "            openai.pydantic_function_tool(Search),\n",
    "            openai.pydantic_function_tool(Memory),\n",
    "        ],\n",
    "    )\n",
    "    choice = ast.literal_eval(response)[0]\n",
    "    print(\"choice\", choice)\n",
    "    return {\n",
    "        \"choice\": choice[\"function_name\"].lower(),\n",
    "        \"input\": {**input, **choice[\"arguments\"]},\n",
    "    }\n",
    "\n",
    "\n",
    "@node(stream=True)\n",
    "def memory(input: dict, callback = None):\n",
    "    print(\"memory\")\n",
    "    query = input[\"query\"]\n",
    "    callback(query)\n",
    "    print(\"memory query\", query)\n",
    "    return {\"input\": {\"text\": \"The user is in Bali right now.\", **input}}\n",
    "\n",
    "\n",
    "@node(stream=True)\n",
    "def search(input: dict, callback = None):\n",
    "    print(\"search\")\n",
    "    query = input[\"query\"]\n",
    "    callback(query)\n",
    "    print(\"search query\", query)\n",
    "    return {\n",
    "        \"input\": {\n",
    "            \"text\": \"The most famous photo spot in Bali is the Uluwatu Temple.\",\n",
    "            **input,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "@node(stream=True)\n",
    "def llm_node(input: dict, callback = None):\n",
    "    print(\"llm_node\")\n",
    "    chat_history = [\n",
    "        Message(role=message[\"role\"], content=message[\"content\"])\n",
    "        for message in input[\"chat_history\"]\n",
    "    ]\n",
    "\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=\"\"\"You are a helpful assistant.\"\"\"),\n",
    "        *chat_history,\n",
    "        Message(\n",
    "            role=\"user\",\n",
    "            content=(\n",
    "                f\"Response to the following query from the user: {input['query']}\\n\"\n",
    "                \"Here is additional context. You can use it to answer the user query. \"\n",
    "                f\"But do not directly reference it: {input.get('text', '')}.\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "    response = llm(messages=messages)\n",
    "    return {\"output\": response}\n",
    "\n",
    "\n",
    "@node(end=True)\n",
    "def node_end(input: dict, callback = None):\n",
    "    \"\"\"Descriptive string for the node.\"\"\"\n",
    "    print(\"node_end\")\n",
    "    return {\"output\": input[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphai import Graph\n",
    "\n",
    "graph = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_fn in [node_start, memory, search, llm_node, node_end]:\n",
    "    graph.add_node(node_fn)\n",
    "\n",
    "# add the router\n",
    "graph.add_router(\n",
    "    source=node_start,\n",
    "    router=node_router,\n",
    "    destinations=[memory, search]\n",
    ")\n",
    "\n",
    "# build the certain edges\n",
    "graph.add_edge(source=node_start, destination=node_router)\n",
    "graph.add_edge(source=memory, destination=llm_node)\n",
    "graph.add_edge(source=search, destination=llm_node)\n",
    "graph.add_edge(source=llm_node, destination=node_end)\n",
    "\n",
    "#graph.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(query: str):\n",
    "    chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "    response = await graph.async_execute(\n",
    "        input={\"input\": {\"query\": query, \"chat_history\": chat_history}}\n",
    "    )\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": response[\"output\"]})\n",
    "    return response[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_start'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_router'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:41 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_a\n",
      "node_router\n",
      "choice {'function_name': 'Memory', 'arguments': {'query': 'user location'}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No callback provided to graph. Please add it via `.add_callback`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m chat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo you remember where I am?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      2\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: query})\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39masync_execute(\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: chat_history}}\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     chat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/projects/aurelio-labs/graphai/graphai/graph.py:67\u001b[0m, in \u001b[0;36mGraph.async_execute\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo callback provided to graph. Please add it via `.add_callback`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# add callback tokens and param here if we are streaming\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback\u001b[38;5;241m.\u001b[39mstart_node(node_name\u001b[38;5;241m=\u001b[39mcurrent_node\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mValueError\u001b[0m: No callback provided to graph. Please add it via `.add_callback`."
     ]
    }
   ],
   "source": [
    "await chat(\"do you remember where I am?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphai.callback import Callback\n",
    "\n",
    "callback = Callback()\n",
    "\n",
    "graph.add_callback(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphai.callback.Callback at 0x11f030550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_start'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_router'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:52 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'do you remember where I am?', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_a\n",
      "node_router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='memory'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: callback=<graphai.callback.Callback object at 0x11f030550>\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='llm_node'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger TEST: input={'input': {'text': 'The user is in Bali right now.', 'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: callback=<graphai.callback.Callback object at 0x11f030550>\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger TEST 2: input={'input': {'text': 'The user is in Bali right now.', 'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:53 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'text': 'The user is in Bali right now.', 'query': 'user location', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice {'function_name': 'Memory', 'arguments': {'query': 'user location'}}\n",
      "memory\n",
      "memory query user location\n",
      "llm_node\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You're currently in Bali. How can I assist you further?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chat(\"do you remember where I am?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_start'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='node_router'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:55 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'tell me a long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_a\n",
      "node_router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='memory'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger TEST: input={'input': {'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: callback=<graphai.callback.Callback object at 0x11f030550>\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger TEST 2: input={'input': {'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: func.__name__='llm_node'\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger TEST: input={'input': {'text': 'The user is in Bali right now.', 'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: stream=True\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: callback=<graphai.callback.Callback object at 0x11f030550>\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger TEST 2: input={'input': {'text': 'The user is in Bali right now.', 'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n",
      "\u001b[32m2024-09-07 19:54:56 INFO semantic_router.utils.logger JB TEMP !!!: args_dict={'input': {'text': 'The user is in Bali right now.', 'query': 'long story', 'chat_history': [{'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'user', 'content': 'do you remember where I am?'}, {'role': 'assistant', 'content': \"You're currently in Bali. How can I assist you further?\"}]}, 'callback': <graphai.callback.Callback object at 0x11f030550>}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice {'function_name': 'Memory', 'arguments': {'query': 'long story'}}\n",
      "memory\n",
      "memory query long story\n",
      "llm_node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-07 19:54:57 INFO semantic_router.utils.logger [*]  Stream Started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<graphai:start:memory>\n",
      "user location\n",
      "<graphai:end:memory>\n",
      "<graphai:start:llm_node>\n",
      "<graphai:end:llm_node>\n",
      "<graphai:start:memory>\n",
      "long story\n",
      "<graphai:end:memory>\n",
      "<graphai:start:llm_node>\n",
      "<graphai:end:llm_node>\n"
     ]
    }
   ],
   "source": [
    "query = \"tell me a long story\"\n",
    "response = await graph.async_execute(\n",
    "    input={\"input\": {\"query\": query, \"chat_history\": chat_history}}\n",
    ")\n",
    "\n",
    "async for token in callback.aiter():\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
